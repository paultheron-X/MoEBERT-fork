## How does this folder work ? 

This folder `sh_scripts` contains all the scripts that are used to train the models and generate the results.

High level overview:

- `python_helpers` contains all the python scripts that are used to:
    - Getting the data hyperparameters (from a csv grid)
    - Aggregating the raw logs that are generated by the training scripts
    - Generating the plots (loss curves for each experiment)

- `experiments` contains all the scripts that are used to train the models
    - High level scripts like `launch_jobs`, or `hyperparameter_metatuner` launch multiple experiments
    - The other scripts are used to launch a single experiment they start with `base_`

- `supercluster` contains the scripts that are used to launch the jobs on the supercluster

- `fetching` contains the scripts that are used to fetch the results from the cluster 

- `notes` contains some notes that I took during the project and to do lists


## How to use it ?

Some commands that are useful to use this folder:

#### Aggregating the logs

- `bash sh_scripts/fetching/fetch_all_results.sh` will fetch all the results from the cluster and put them in the `results_gcloud` folder. It works with the following options:
    - `all` will fetch all the results for trainings **without moebert (bert_base)**
    - `allb` will fetch all the results for trainings **with moebert**

`bash sh_scripts/fetching/fetch_all_results.sh allb`

#### Launching jobs

- `bash sh_scripts/experiments/launch_jobs.sh $1 $2` will launch jobs on the cluster. It works with the following options:
    - `$1` is the name of the experiment (e.g. `cola`, `mnli`, `sst2` ...)
    - `$2` is the batch of hyperparameters that you want to launch (e.g. `1`, `2` ... `6`)
    - **Note**: the hyperparameters are defined in the `sh_scripts/python_helpers` folder, for some small datasets (e.g. `cola`) there is only one batch of hyperparameters and everything will be launched with `$2=1`

`bash sh_scripts/experiments/launch_jobs.sh cola 1`

- `bash sh_scripts/experiments/launch_jobs.sh $1` will launch 5 jobs with more epochs on the cluster. It works with the following options:
    - `$1` is the name of the experiment (works only with big ds) (e.g. `qqp`, `mnli`, `sst2` ...)
    - **Note**: The 5 jobs are launched with good performing hyperparameters that were found in a previous hyperparameter tuning