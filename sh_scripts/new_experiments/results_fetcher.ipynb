{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = ['squad', 'squad_v2'] #[\"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", 'squad', 'squad_v2']\n",
    "\n",
    "dirs_per_task_topk = defaultdict(list)\n",
    "dirs_per_task_trimmed = defaultdict(list)\n",
    "\n",
    "for task in glue:\n",
    "    for dirs in os.listdir(f\"../../results/{task}\"):\n",
    "        if dirs.startswith(\"new_moebert_k2_experiment\"):\n",
    "            #print(dirs)\n",
    "            if 'trl_' in dirs:\n",
    "                dirs_per_task_trimmed[task].append(f\"../../results/{task}/{dirs}\")\n",
    "            else:\n",
    "                # add the full path to the list\n",
    "                dirs_per_task_topk[task].append(f\"../../results/{task}/{dirs}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(list, metric_name):\n",
    "    best_metric = 0\n",
    "    best_epoch = 0\n",
    "    for item_dict in list:\n",
    "        if f'eval_{metric_name}' in item_dict:\n",
    "            if item_dict[f'eval_{metric_name}'] > best_metric:\n",
    "                best_metric = item_dict[f'eval_{metric_name}']\n",
    "                best_epoch = item_dict['epoch']\n",
    "    return best_metric, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with task, dis, wdec given the path\n",
    "# for each of them, get the best metric, given the model in the path\n",
    "\n",
    "metric_for_task = {\n",
    "    \"cola\": \"matthews_correlation\",\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mrpc\": \"accuracy\",\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": \"f1\",\n",
    "    \"rte\": \"accuracy\",\n",
    "    \"sst2\": \"accuracy\",\n",
    "    \"squad\" : \"f1\",\n",
    "    \"squad_v2\": \"f1\",\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(columns=[\"task\", \"dis\", \"wdec\", \"value\", \"epoch\", \"seed\"])\n",
    "\n",
    "for task in glue:\n",
    "    for path in dirs_per_task_topk[task]:\n",
    "        #print(path)\n",
    "        try:\n",
    "            with open(f\"{path}/model/trainer_state.json\", \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            best_result, best_epoch = get_best_metric(results['log_history'], metric_for_task[task])\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                with open(f\"{path}/all_results.json\", \"r\") as f:\n",
    "                    best_epoch = 10\n",
    "                    best_result = json.load(f)[metric_for_task[task]]\n",
    "            except FileNotFoundError:\n",
    "                best_result = 0\n",
    "                best_epoch = 0\n",
    "        dis = path.split(\"/\")[-1].split(\"_\")[5]\n",
    "        wdec = path.split(\"/\")[-1].split(\"_\")[7]\n",
    "        try:\n",
    "            seed = path.split(\"/\")[-1].split(\"_\")[9]\n",
    "        except:\n",
    "            seed = \"0\"\n",
    "        df_results = df_results.append({\"task\": task, \"dis\": dis, \"wdec\": wdec, \"value\": best_result, \"epoch\": best_epoch, \"seed\": seed}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of experiments per task\n",
    "print(df_results.groupby(\"task\").count()['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best value for each task, and the corresponding dis and wdec and epoch\n",
    "best_results = pd.DataFrame(columns=[\"task\", \"dis\", \"wdec\", \"value\", \"epoch\", \"seed\"])\n",
    "for task in glue:\n",
    "    best_results = best_results.append(df_results[df_results[\"task\"] == task].sort_values(by=\"value\", ascending=False).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimmed Lasso Gate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_trimmed = pd.DataFrame(columns=[\"task\", \"dis\", \"wdec\", \"trl\", \"value\", \"epoch\", \"seed\"])\n",
    "\n",
    "for task in glue:\n",
    "    for path in dirs_per_task_trimmed[task]:\n",
    "        try:\n",
    "            with open(f\"{path}/model/trainer_state.json\", \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            best_result, best_epoch = get_best_metric(results['log_history'], metric_for_task[task])\n",
    "            print(best_result, best_epoch)\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                with open(f\"{path}/all_results.json\", \"r\") as f:\n",
    "                    best_epoch = 10\n",
    "                    best_result = json.load(f)[metric_for_task[task]]\n",
    "            except FileNotFoundError:\n",
    "                best_result = 0\n",
    "                best_epoch = 0\n",
    "        dis = path.split(\"/\")[-1].split(\"_\")[5]\n",
    "        wdec = path.split(\"/\")[-1].split(\"_\")[7]\n",
    "        trl = path.split(\"/\")[-1].split(\"_\")[9]\n",
    "        try:\n",
    "            seed = path.split(\"/\")[-1].split(\"_\")[11]\n",
    "        except:\n",
    "            seed = \"0\"\n",
    "        df_results_trimmed = df_results_trimmed.append({\"task\": task, \"dis\": dis, \"wdec\": wdec, \"trl\": trl, \"value\": best_result, \"epoch\": best_epoch, \"seed\": seed}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results_trimmed.groupby(\"task\").count()['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best value for each task, and the corresponding dis and wdec and epoch\n",
    "best_results_trimmed = pd.DataFrame(columns=[\"task\", \"dis\", \"wdec\", \"trl\", \"value\", \"epoch\", \"seed\"])\n",
    "for task in glue:\n",
    "    try:\n",
    "        best_results_trimmed = best_results_trimmed.append(df_results_trimmed[df_results_trimmed[\"task\"] == task].sort_values(by=\"value\", ascending=False).iloc[0], ignore_index=True)\n",
    "    except IndexError:\n",
    "        #print(task)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_results_trimmed.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results[best_results['task'] != 'mnli'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winning models (when we do better than the best baseline)\n",
    "best_results_trimmed[best_results_trimmed['value'] >= best_results[best_results['task'] != 'mnli'].reset_index(drop=True)['value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_trimmed = pd.DataFrame(columns=[\"task\", \"dis\", \"wdec\", \"trl\", \"value\", \"epoch\", \"seed\"])\n",
    "\n",
    "for task in glue:\n",
    "    for path in dirs_per_task_trimmed[task]:\n",
    "        try:\n",
    "            with open(f\"{path}/model/all_results.json\", \"r\") as f:\n",
    "                best_epoch = 10\n",
    "                best_result = json.load(f)[metric_for_task[task]]\n",
    "        except FileNotFoundError:\n",
    "            best_result = 0\n",
    "            best_epoch = 0\n",
    "        dis = path.split(\"/\")[-1].split(\"_\")[5]\n",
    "        wdec = path.split(\"/\")[-1].split(\"_\")[7]\n",
    "        trl = path.split(\"/\")[-1].split(\"_\")[9]\n",
    "        try:\n",
    "            seed = path.split(\"/\")[-1].split(\"_\")[11]\n",
    "        except:\n",
    "            seed = \"0\"\n",
    "        df_results_trimmed = df_results_trimmed.append({\"task\": task, \"dis\": dis, \"wdec\": wdec, \"trl\": trl, \"value\": best_result, \"epoch\": best_epoch, \"seed\": seed}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_trimmed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoEBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
